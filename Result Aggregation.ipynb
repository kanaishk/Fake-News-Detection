{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b2302-1d9a-4b11-a01e-9e0fe9773f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from re import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807048a-ffd4-42dc-be3d-4d581f7022a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc28af4-3567-40b7-bcae-962bd7c7ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e98b1c-5f08-46ca-9c35-c401ec21b3b2",
   "metadata": {},
   "source": [
    "<h3>Importing the dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867905d-d70b-48eb-8549-55ea02413482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "dataset_dir = os.path.join(cwd,'Dataset')\n",
    "result_dir = os.path.join(cwd,'Results')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7755d2e5-e935-4be4-9213-463091c0faff",
   "metadata": {},
   "source": [
    "Data- Description: There are 11 columns in the dataset provided to you. The description of each of the column is given below: \n",
    "“id”: Unique id of each news article \n",
    "“headline”: It is the title of the news. \n",
    "“written_by”: It represents the author of the news article\n",
    "“news”: It contains the full text of the news article \n",
    "“label”: It tells whether the news is fake (1) or not fake (0).\n",
    "\"headline_len\": length of headline\n",
    "\"news_len\": length of news article\n",
    "\"caps_in_headline\": number of capital letter in headline\n",
    "\"norm_caps_in_headline\": percentage of capital in headline\n",
    "\"caps_in_news\": number of capital letter in news article\n",
    "\"norm_caps_in_news\": percentage of capital in news article\n",
    "\"news_tokens\": tokenized news article\n",
    "\"news_url\": urls in news article\n",
    "\"clean_news\": cleaned up news article\n",
    "\"headline_url\": urls in headlines\n",
    "\"clean_headline\": cleaned up news headline\n",
    "\"twitter_handles\": twitter handles in news article\n",
    "\"clean_news_tokens\": tokens of cleaned news article\n",
    "\"clean_headline_tokens\": tokens of cleaned headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de9c07-3776-443b-ad15-a1291493401c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelname = ['GBC','LR','MNB','RFC','SVM']\n",
    "modelnamebase = ['GBC','LR','MNB','RFC','SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacb3d0-6727-4d98-8aac-fa9bdf0c14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = []\n",
    "train_res = []\n",
    "valid_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8cdec-042e-4e79-aeeb-0167cb0c26bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_test_res = []\n",
    "base_train_res = []\n",
    "base_valid_res = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12aa39-db28-4591-bb7e-1e39b72c5748",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Result Processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff373b-7c20-4665-b034-170c8e8b1f93",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Base</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e2151-40c1-4f73-8ea6-3a2e5ed98fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in modelnamebase:\n",
    "    base_train_df = pd.read_csv(os.path.join(result_dir, f'BASE\\BASE_{model}_Train_result.csv'))\n",
    "    base_test_df = pd.read_csv(os.path.join(result_dir, f'BASE\\BASE_{model}_Test_result.csv'))\n",
    "    \n",
    "    unnamed_columns = [col for col in base_test_df.columns if search(r'^Unnamed', col)]\n",
    "    base_test_df = base_test_df.drop(unnamed_columns, axis=1)\n",
    "    base_test_df.drop('Time (Sec)', axis=1, inplace=True)\n",
    "    \n",
    "    unnamed_columns = [col for col in base_train_df.columns if search(r'^Unnamed', col)]\n",
    "    base_train_df = base_train_df.drop(unnamed_columns, axis=1)\n",
    "    base_train_df.drop('Time (Sec)', axis=1, inplace=True)\n",
    "    \n",
    "    base_test_df.drop('ID', axis=1, inplace=True)\n",
    "    base_train_df.drop('ID', axis=1, inplace=True)\n",
    "    \n",
    "    base_final_test_res = base_test_df.to_dict(orient='records')[0]\n",
    "    base_final_test_res['Model'] = model\n",
    "    base_test_res.append(base_final_test_res)\n",
    "    \n",
    "    base_final_train_res = base_train_df.to_dict(orient='records')[0]\n",
    "    base_final_train_res['Model'] = model\n",
    "    base_train_res.append(base_final_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51c9f5-add0-4f5d-8c6a-2c6ab4dc4ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = 'Voting'\n",
    "base_train_df = pd.read_csv(os.path.join(result_dir, f'BASE\\BASE_{model}_Train_result.csv'))\n",
    "base_test_df = pd.read_csv(os.path.join(result_dir, f'BASE\\BASE_{model}_Test_result.csv'))\n",
    "\n",
    "unnamed_columns = [col for col in base_test_df.columns if search(r'^Unnamed', col)]\n",
    "base_test_df = base_test_df.drop(unnamed_columns, axis=1)\n",
    "base_test_df.drop('Time (Sec)', axis=1, inplace=True)\n",
    "\n",
    "unnamed_columns = [col for col in base_train_df.columns if search(r'^Unnamed', col)]\n",
    "base_train_df = base_train_df.drop(unnamed_columns, axis=1)\n",
    "base_train_df.drop('Time (Sec)', axis=1, inplace=True)\n",
    "\n",
    "base_final_test_res = base_test_df.to_dict(orient='records')[0]\n",
    "base_final_test_res['Model'] = model\n",
    "base_test_res.append(base_final_test_res)\n",
    "\n",
    "base_final_train_res = base_train_df.to_dict(orient='records')[0]\n",
    "base_final_train_res['Model'] = model\n",
    "base_train_res.append(base_final_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821eac7f-2eb6-4b17-9446-32e65704452c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelnamebase.append('Voting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012b300-c8af-40a9-b817-5e5dba6302dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in modelnamebase :\n",
    "    file_path = os.path.join(result_dir,f'BASE\\BASE_{model}_Valid_report.txt')\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        classification_report_str = file.read()\n",
    "\n",
    "    lines = classification_report_str.strip().split('\\n')\n",
    "\n",
    "    accuracy_line = lines[-3].split()[1:]\n",
    "    macro_avg_line = lines[-2].split()[2:-1]\n",
    "\n",
    "    accuracy = float(accuracy_line[0])\n",
    "    macro_avg_precision, macro_avg_recall, macro_avg_f1 = map(float, macro_avg_line)\n",
    "    \n",
    "    base_valid_res_mdl = dict()\n",
    "    base_valid_res_mdl['Accuracy'] = accuracy\n",
    "    base_valid_res_mdl['F1-Score'] = macro_avg_f1\n",
    "    base_valid_res_mdl['Recall'] = macro_avg_recall\n",
    "    base_valid_res_mdl['Precision'] = macro_avg_precision\n",
    "    base_valid_res_mdl['Model']=model\n",
    "    \n",
    "    base_valid_res.append(base_valid_res_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4f6ae-fd60-4400-a607-e7aeffa6212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_res_df = pd.DataFrame(base_test_res)\n",
    "base_test_res_df['Model'] = base_test_res_df['Model'].astype(str)\n",
    "base_test_res_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358c564-a4a3-4cf1-b060-2b986f19ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_res_df = pd.DataFrame(base_train_res)\n",
    "base_train_res_df['Model'] = base_train_res_df['Model'].astype(str)\n",
    "base_train_res_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892d7ea-1348-404f-8343-b26298627e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_valid_res_df = pd.DataFrame(base_valid_res)\n",
    "base_valid_res_df['Model'] = base_valid_res_df['Model'].astype(str)\n",
    "base_valid_res_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3005ffd-924e-41cf-84aa-8858ff53a96f",
   "metadata": {},
   "source": [
    "<h4>Hyper Tuned</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e90ae-7d88-4227-aafa-2aea72cc4f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in modelname :\n",
    "    train_df = pd.read_csv(os.path.join(result_dir,f'{model}_Train_result.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(result_dir,f'{model}_Test_result.csv'))\n",
    "    parameter_df = pd.read_csv(os.path.join(result_dir,f'{model}_Parameters.csv'))\n",
    "    \n",
    "    unnamed_columns = [col for col in test_df.columns if search(r'^Unnamed', col)]\n",
    "    test_df = test_df.drop(unnamed_columns, axis=1)\n",
    "    test_columns = list(test_df.columns)\n",
    "    \n",
    "    test_top_ten = pd.DataFrame()\n",
    "    test_top_ten = test_top_ten.append(test_df.nlargest(1, ['Accuracy']), ignore_index = True)\n",
    "    test_top_ten.columns = test_columns\n",
    "    test_top_ten.drop('Time (Sec)', axis=1, inplace=True)\n",
    "    '''\n",
    "    print('Test')\n",
    "    print(test_top_ten)\n",
    "    '''\n",
    "    ID_list = list(test_top_ten['ID'])\n",
    "    \n",
    "    unnamed_columns = [col for col in train_df.columns if search(r'^Unnamed', col)]\n",
    "    train_df = train_df.drop(unnamed_columns, axis=1)\n",
    "    train_columns = list(train_df.columns)\n",
    "    \n",
    "    train_ten = pd.DataFrame()\n",
    "    for ID in ID_list :\n",
    "        train_ten = train_ten.append([train_df[train_df.ID == ID]], ignore_index = True)\n",
    "    train_ten.columns = train_columns\n",
    "    train_ten.drop('Time (Sec)', axis=1, inplace=True)\n",
    "    '''\n",
    "    print('Train')\n",
    "    print(train_ten)\n",
    "    '''\n",
    "    unnamed_columns = [col for col in parameter_df.columns if search(r'^Unnamed', col)]\n",
    "    parameter_df = parameter_df.drop(unnamed_columns, axis=1)\n",
    "    parameter_columns = list(parameter_df.columns)\n",
    "    '''\n",
    "    parameter_ten = pd.DataFrame()\n",
    "    for ID in ID_list :\n",
    "        parameter_ten = parameter_ten.append([parameter_df[parameter_df.ID == ID]], ignore_index = True)\n",
    "    parameter_ten.columns = parameter_columns\n",
    "    \n",
    "    print(f'Parameters {model}')\n",
    "    print(parameter_ten)\n",
    "    print('\\n')\n",
    "    '''\n",
    "    final_test_res = dict(test_top_ten.mean())\n",
    "    final_test_res['Model'] = model\n",
    "    test_res.append(final_test_res)\n",
    "    \n",
    "    final_train_res = dict(train_ten.mean())\n",
    "    final_train_res['Model'] = model\n",
    "    train_res.append(final_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0a77f-bf74-4fae-970b-a5220ee51fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = 'Voting'\n",
    "train_df = pd.read_csv(os.path.join(result_dir,f'{model}_Train_result.csv'))\n",
    "test_df = pd.read_csv(os.path.join(result_dir,f'{model}_Test_result.csv'))\n",
    "\n",
    "unnamed_columns = [col for col in test_df.columns if search(r'^Unnamed', col)]\n",
    "test_df = test_df.drop(unnamed_columns, axis=1)\n",
    "test_columns = list(test_df.columns)\n",
    "\n",
    "test_top_ten = pd.DataFrame()\n",
    "test_top_ten = test_top_ten.append(test_df, ignore_index = True)\n",
    "test_top_ten.columns = test_columns\n",
    "test_top_ten.drop('Time (Sec)', axis=1, inplace=True)\n",
    "\n",
    "print('Test')\n",
    "print(test_top_ten)\n",
    "\n",
    "unnamed_columns = [col for col in train_df.columns if search(r'^Unnamed', col)]\n",
    "train_df = train_df.drop(unnamed_columns, axis=1)\n",
    "train_columns = list(train_df.columns)\n",
    "\n",
    "train_ten = pd.DataFrame()\n",
    "train_ten = train_ten.append(train_df, ignore_index = True)\n",
    "train_ten.columns = train_columns\n",
    "train_ten.drop('Time (Sec)', axis=1, inplace=True)\n",
    "\n",
    "print('Train')\n",
    "print(train_ten)\n",
    "\n",
    "final_test_res = dict(test_top_ten.mean())\n",
    "final_test_res['Model'] = model\n",
    "test_res.append(final_test_res)\n",
    "\n",
    "final_train_res = dict(train_ten.mean())\n",
    "final_train_res['Model'] = model\n",
    "train_res.append(final_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5842321-81e3-4a8d-8bbd-932421c0100a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelname.append('Voting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d1a57-cf17-4bdf-88e4-3e188052869f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in modelname :\n",
    "    file_path = os.path.join(result_dir,f'{model}_Valid_report.txt')\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        classification_report_str = file.read()\n",
    "\n",
    "    lines = classification_report_str.strip().split('\\n')\n",
    "\n",
    "    accuracy_line = lines[-3].split()[1:]\n",
    "    macro_avg_line = lines[-2].split()[2:-1]\n",
    "\n",
    "    accuracy = float(accuracy_line[0])\n",
    "    macro_avg_precision, macro_avg_recall, macro_avg_f1 = map(float, macro_avg_line)\n",
    "    \n",
    "    valid_res_mdl = dict()\n",
    "    valid_res_mdl['Accuracy'] = accuracy\n",
    "    valid_res_mdl['F1-Score'] = macro_avg_f1\n",
    "    valid_res_mdl['Recall'] = macro_avg_recall\n",
    "    valid_res_mdl['Precision'] = macro_avg_precision\n",
    "    valid_res_mdl['Model']=model\n",
    "    \n",
    "    valid_res.append(valid_res_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e3da2-77dc-49c3-b46e-33681ffcf66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_df = pd.DataFrame(test_res)\n",
    "test_res_df['Model'] = test_res_df['Model'].astype(str)\n",
    "test_res_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efbb293-6bba-4d12-b824-0c7ed450b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_df = pd.DataFrame(train_res)\n",
    "train_res_df['Model'] = train_res_df['Model'].astype(str)\n",
    "train_res_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37770608-00d7-4fe1-8378-5592ae40aeec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_res_df = pd.DataFrame(valid_res)\n",
    "valid_res_df['Model'] = valid_res_df['Model'].astype(str)\n",
    "valid_res_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d282e9-5f2e-4175-ad06-ac19fb1704b9",
   "metadata": {},
   "source": [
    "<h3>Graphing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469871d-442b-412b-a979-827c6ae32d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_dir = os.path.join(cwd,'Graphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ea402-66f1-4f1d-adcc-826b3b2b0bf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Base</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77484eac-0110-492a-b3e7-73644855e7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=base_train_res_df)\n",
    "title='Training Accuracy of Base Models'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6276e-6847-4eef-a471-50601ac1fea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=base_test_res_df)\n",
    "title = 'Base Test Accuracy of Base Models'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c44e90-9701-4dc2-9b0e-b97cc4117ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=base_valid_res_df)\n",
    "title='Base Validation Accuracy of Base Models'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd4ba8-48a2-4a60-a831-5b96baa8b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_base_df = pd.merge(base_train_res_df, base_test_res_df, on='Model', suffixes=('_train', '_test'))\n",
    "merged_base_df = pd.merge(merged_base_df, base_valid_res_df, on='Model')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(merged_base_df['Model'], merged_base_df['Accuracy_train'], marker='o', label='Training Accuracy')\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['Accuracy_train']):\n",
    "    plt.text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.plot(merged_base_df['Model'], merged_base_df['Accuracy_test'], marker='o', label='Testing Accuracy')\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['Accuracy_test']):\n",
    "    plt.text(x, y-0.011, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.plot(merged_base_df['Model'], merged_base_df['Accuracy'], marker='o', label='Validation Accuracy')\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['Accuracy']):\n",
    "    plt.text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "title = 'Base Model Accuracy Comparison'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061b709-61dd-4bf5-8b11-acb5a13703f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Accuracy subgraph\n",
    "axes[0, 0].plot(merged_base_df['Model'], merged_base_df['Accuracy_train'], marker='o', label='Training Accuracy')\n",
    "axes[0, 0].plot(merged_base_df['Model'], merged_base_df['Accuracy_test'], marker='o', label='Testing Accuracy')\n",
    "axes[0, 0].plot(merged_base_df['Model'], merged_base_df['Accuracy'], marker='o', label='Validation Accuracy')\n",
    "axes[0, 0].set_title('Accuracy')\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['Accuracy']):\n",
    "    axes[0, 0].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Precision subgraph\n",
    "axes[0, 1].plot(merged_base_df['Model'], merged_base_df['Precision_train'], marker='o', label='Training Precision')\n",
    "axes[0, 1].plot(merged_base_df['Model'], merged_base_df['Precision_test'], marker='o', label='Testing Precision')\n",
    "axes[0, 1].plot(merged_base_df['Model'], merged_base_df['Precision'], marker='o', label='Validation Precision')\n",
    "axes[0, 1].set_title('Precision')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].legend()\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['Precision']):\n",
    "    axes[0, 1].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Recall subgraph\n",
    "axes[1, 0].plot(merged_base_df['Model'], merged_base_df['Recall_train'], marker='o', label='Training Recall')\n",
    "axes[1, 0].plot(merged_base_df['Model'], merged_base_df['Recall_test'], marker='o', label='Testing Recall')\n",
    "axes[1, 0].plot(merged_base_df['Model'], merged_base_df['Recall'], marker='o', label='Validation Recall')\n",
    "axes[1, 0].set_title('Recall')\n",
    "axes[1, 0].set_xlabel('Model')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].legend()\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['Recall']):\n",
    "    axes[1, 0].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# F1 Score subgraph\n",
    "axes[1, 1].plot(merged_base_df['Model'], merged_base_df['F1-Score_train'], marker='o', label='Training F1 Score')\n",
    "axes[1, 1].plot(merged_base_df['Model'], merged_base_df['F1-Score_test'], marker='o', label='Testing F1 Score')\n",
    "axes[1, 1].plot(merged_base_df['Model'], merged_base_df['F1-Score'], marker='o', label='Validation F1 Score')\n",
    "axes[1, 1].set_title('F1 Score')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].legend()\n",
    "for x, y in zip(merged_base_df['Model'], merged_base_df['F1-Score']):\n",
    "    axes[1, 1].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save and display the graph\n",
    "title = 'Base Model Performance Comparison'\n",
    "plt.suptitle(title, fontsize=16)\n",
    "plt.savefig(os.path.join(graph_dir, title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03d668-d45a-4612-948f-017bbfdc22c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Hyper Tuned</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e31dd-9d07-43d6-91f9-cd6e98684d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=train_res_df)\n",
    "title='Training Accuracy of Hyperparameter Tuned Model'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2ef6f-3c51-481b-8bf8-3102a7d3a509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=test_res_df)\n",
    "title = 'Test Accuracy of Hyperparameter Tuned Model'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98267-1d8f-4506-9ae2-d55efc800882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=valid_res_df)\n",
    "title='Validation Accuracy of Hyperparameter Tuned Model'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f318e1-2d15-4814-9462-c01d415eb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tuned_df = pd.merge(train_res_df, test_res_df, on='Model', suffixes=('_train', '_test'))\n",
    "merged_tuned_df = pd.merge(merged_tuned_df, valid_res_df, on='Model')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(merged_tuned_df['Model'], merged_tuned_df['Accuracy_train'], marker='o', label='Training Accuracy')\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['Accuracy_train']):\n",
    "    plt.text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.plot(merged_tuned_df['Model'], merged_tuned_df['Accuracy_test'], marker='o', label='Testing Accuracy')\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['Accuracy_test']):\n",
    "    plt.text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.plot(merged_tuned_df['Model'], merged_tuned_df['Accuracy'], marker='o', label='Validation Accuracy')\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['Accuracy']):\n",
    "    plt.text(x, y-0.005, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "title='Tuned Model Accuracy Comparison'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(graph_dir,title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa3c0f-3897-4fbf-bc11-4afb18841af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Accuracy subgraph\n",
    "axes[0, 0].plot(merged_tuned_df['Model'], merged_tuned_df['Accuracy_train'], marker='o', label='Training Accuracy')\n",
    "axes[0, 0].plot(merged_tuned_df['Model'], merged_tuned_df['Accuracy_test'], marker='o', label='Testing Accuracy')\n",
    "axes[0, 0].plot(merged_tuned_df['Model'], merged_tuned_df['Accuracy'], marker='o', label='Validation Accuracy')\n",
    "axes[0, 0].set_title('Accuracy')\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['Accuracy']):\n",
    "    axes[0, 0].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Precision subgraph\n",
    "axes[0, 1].plot(merged_tuned_df['Model'], merged_tuned_df['Precision_train'], marker='o', label='Training Precision')\n",
    "axes[0, 1].plot(merged_tuned_df['Model'], merged_tuned_df['Precision_test'], marker='o', label='Testing Precision')\n",
    "axes[0, 1].plot(merged_tuned_df['Model'], merged_tuned_df['Precision'], marker='o', label='Validation Precision')\n",
    "axes[0, 1].set_title('Precision')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].legend()\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['Precision']):\n",
    "    axes[0, 1].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Recall subgraph\n",
    "axes[1, 0].plot(merged_tuned_df['Model'], merged_tuned_df['Recall_train'], marker='o', label='Training Recall')\n",
    "axes[1, 0].plot(merged_tuned_df['Model'], merged_tuned_df['Recall_test'], marker='o', label='Testing Recall')\n",
    "axes[1, 0].plot(merged_tuned_df['Model'], merged_tuned_df['Recall'], marker='o', label='Validation Recall')\n",
    "axes[1, 0].set_title('Recall')\n",
    "axes[1, 0].set_xlabel('Model')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].legend()\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['Recall']):\n",
    "    axes[1, 0].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# F1 Score subgraph\n",
    "axes[1, 1].plot(merged_tuned_df['Model'], merged_tuned_df['F1-Score_train'], marker='o', label='Training F1 Score')\n",
    "axes[1, 1].plot(merged_tuned_df['Model'], merged_tuned_df['F1-Score_test'], marker='o', label='Testing F1 Score')\n",
    "axes[1, 1].plot(merged_tuned_df['Model'], merged_tuned_df['F1-Score'], marker='o', label='Validation F1 Score')\n",
    "axes[1, 1].set_title('F1 Score')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].legend()\n",
    "for x, y in zip(merged_tuned_df['Model'], merged_tuned_df['F1-Score']):\n",
    "    axes[1, 1].text(x, y, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save and display the graph\n",
    "title = 'Tuned Model Performance Comparison'\n",
    "plt.suptitle(title, fontsize=16)\n",
    "plt.savefig(os.path.join(graph_dir, title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242fd60-575b-4ab5-a6a3-1f2fb5a8d34e",
   "metadata": {},
   "source": [
    "<h3>Others</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdd196-0407-4d2f-a70f-01c1ecad5185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the difference in accuracies between base and tuned models\n",
    "merged_diff_df = pd.DataFrame()\n",
    "merged_diff_df['Model'] = merged_base_df['Model']\n",
    "merged_diff_df['Diff_Accuracy_train'] = merged_tuned_df['Accuracy_train'] - merged_base_df['Accuracy_train']\n",
    "merged_diff_df['Diff_Accuracy_test'] = merged_tuned_df['Accuracy_test'] - merged_base_df['Accuracy_test']\n",
    "merged_diff_df['Diff_Accuracy'] = merged_tuned_df['Accuracy'] - merged_base_df['Accuracy']\n",
    "\n",
    "# Plot the difference in accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(merged_diff_df['Model'], merged_diff_df['Diff_Accuracy_train'], marker='o', label='Training Accuracy')\n",
    "for x, y in zip(merged_diff_df['Model'], merged_diff_df['Diff_Accuracy_train']):\n",
    "    plt.text(x, y+0.002, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.plot(merged_diff_df['Model'], merged_diff_df['Diff_Accuracy_test'], marker='o', label='Testing Accuracy')\n",
    "for x, y in zip(merged_diff_df['Model'], merged_diff_df['Diff_Accuracy_test']):\n",
    "    plt.text(x, y-0.008, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.plot(merged_diff_df['Model'], merged_diff_df['Diff_Accuracy'], marker='o', label='Validation Accuracy')\n",
    "for x, y in zip(merged_diff_df['Model'], merged_diff_df['Diff_Accuracy']):\n",
    "    plt.text(x, y-0.015, f'{y:.4f}', ha='center', va='bottom')\n",
    "\n",
    "title = 'Difference in Accuracy-Base vs Tuned Models'\n",
    "plt.title(title)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy Difference')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(graph_dir, title+'.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ada7b-4160-46a6-8f75-b47c39bcff7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
